#libraries
!pip install datasets
from pandas import read_csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from datasets import ClassLabel,Sequence
import random
from IPython.display import HTML
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import gensim.downloader as api
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from gensim.models import Word2Vec
import multiprocessing
from gensim.models import FastText
import multiprocessing
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, TimeDistributed, Dense

from datasets import load_dataset

datasets_CW = load_dataset("surrey-nlp/PLOD-CW")


# Accessing the train, test, and validation splits
train = datasets_CW['train']
test = datasets_CW['test']
validation = datasets_CW['validation']
#validation

# Accessing the tokens, pos_tags, and ner_tags columns from the training data
train_tokens = train["tokens"]
train_pos_tags = train["pos_tags"]
train_ner_tags = train["ner_tags"]

from itertools import chain

# Unique values in train_ner_tags
unique_ner_tags = set(chain.from_iterable(train_ner_tags))

print("Unique values NER:", unique_ner_tags)

# Accessing the tokens, pos_tags, and ner_tags columns from the testing data
test_tokens = test["tokens"]
test_pos_tags = test["pos_tags"]
test_ner_tags = test["ner_tags"]

# Accessing the tokens, pos_tags, and ner_tags columns from the validation data
validation_tokens = validation["tokens"]
validation_pos_tags = validation["pos_tags"]
validation_ner_tags = validation["ner_tags"]
#validation_ner_tags

datasets_CW

# Observing the first 5 rows of the training data in the dataset
def show_random_elements(dataset, num_examples=5):
    assert num_examples <= len(dataset)
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
show_random_elements(datasets_CW["train"])


import matplotlib.pyplot as plt

size_train = len(train['tokens'])
size_validation = len(validation['tokens'])
size_test = len(test['tokens'])

# Dataset names
datasets = ['Train', 'Validation', 'Test']

# Sizes of each dataset
sizes = [size_train, size_validation, size_test]

# Creating the bar graph
plt.figure(figsize=(10, 6))
plt.bar(datasets, sizes, color=['blue', 'green', 'red'])
plt.xlabel('Dataset')
plt.ylabel('Number of Tokens')
plt.title('Number of Tokens in Each Dataset')
plt.show()


import matplotlib.pyplot as plt

# Replace these with the actual lengths of your datasets
len_train = len(train["tokens"])
len_test = len(test["tokens"])
len_validation = len(validation["tokens"])
sizes = [len_train, len_test, len_validation]
labels = 'Train', 'Test', 'Validation'

# Pie chart, where the slices will be ordered and plotted counter-clockwise:
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.show()


import matplotlib.pyplot as plt
from collections import Counter
from itertools import chain

# FlattenIing the list of NER tags from each dataset
flat_train_ner = list(chain.from_iterable(train['ner_tags']))
flat_test_ner = list(chain.from_iterable(test['ner_tags']))
flat_validation_ner = list(chain.from_iterable(validation['ner_tags']))

# Counting the occurrences of each NER tag in each dataset
train_ner_counts = Counter(flat_train_ner)
test_ner_counts = Counter(flat_test_ner)
validation_ner_counts = Counter(flat_validation_ner)

# Getting a sorted list of unique NER tags across all datasets
all_ner_tags = sorted(set(train_ner_counts.keys()) | set(test_ner_counts.keys()) | set(validation_ner_counts.keys()))

# Calculating the count of each NER tag for each dataset
train_tag_counts = [train_ner_counts.get(tag, 0) for tag in all_ner_tags]
test_tag_counts = [test_ner_counts.get(tag, 0) for tag in all_ner_tags]
validation_tag_counts = [validation_ner_counts.get(tag, 0) for tag in all_ner_tags]

# Setting up the bar chart
bar_width = 0.25  # Width of the bars
index = range(len(all_ner_tags))  # The x locations for the groups

# Creating the plot
plt.figure(figsize=(10, 6))
plt.bar(index, train_tag_counts, bar_width, label='Train')
plt.bar([i + bar_width for i in index], test_tag_counts, bar_width, label='Test')
plt.bar([i + bar_width * 2 for i in index], validation_tag_counts, bar_width, label='Validation')

# Adding labels and title
plt.xlabel('NER Tags')
plt.ylabel('Counts')
plt.title('Distribution of NER Tags Across Datasets')
plt.xticks([i + bar_width for i in index], all_ner_tags, rotation=45)
plt.legend()

# Displaying the plot
plt.tight_layout()  # Adjust layout to fit all labels
plt.show()


# Dropping rows with missing values
train = train.filter(lambda example: example['tokens'] is not None)
validation = validation.filter(lambda example: example['tokens'] is not None)
test = test.filter(lambda example: example['tokens'] is not None)


# Applying fill-in-the-blanks with the most frequent value
def clean_and_transform(example):
    # We need the pos_tags and ner_tags columns
    # We will calculate the most frequent value and replace None (example)
    for column in ['pos_tags', 'ner_tags']:
        most_common = pd.Series([x for sublist in datasets_CW['train'][column] for x in sublist]).mode()[0]
        example[column] = [most_common if v is None else v for v in example[column]]
    return example

# Applying the function to all splits
datasets_CW = datasets_CW.map(clean_and_transform)


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

# Definition of preprocessing functions
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_tokens(example):
    # Tokenization
    tokens = word_tokenize(' '.join(example['tokens']))

    # Normalization
    tokens = [re.sub(r'[^A-Za-z0-9]', '', token) for token in tokens]

    # Stopwords Cleaning
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Removing empty strings
    tokens = [token for token in tokens if token.strip()]

    example['processed_tokens'] = tokens
    return example


# Application of preprocessing functions
train = train.map(preprocess_tokens)
validation = validation.map(preprocess_tokens)
test = test.map(preprocess_tokens)

# Display the first lines from each set
print("Train set processed tokens example:", train['processed_tokens'][:3])
print("Validation set processed tokens example:", validation['processed_tokens'][:3])
print("Test set processed tokens example:", test['processed_tokens'][:3])

print(train)
print(validation)
print(test)

# Delete original 'tokens'
train = train.remove_columns('tokens')
validation = validation.remove_columns('tokens')
test = test.remove_columns('tokens')
# Print to confirm the process
print(train)
print(validation)
print(test)

from datasets import DatasetDict

# Replace 'tokens' with 'processed tokens' and rename to 'tokens'
def replace_and_rename_tokens(example):
    example['tokens'] = example.pop('processed_tokens')
    return example

# Apply the change to each dataset
train = train.map(replace_and_rename_tokens)
validation = validation.map(replace_and_rename_tokens)
test = test.map(replace_and_rename_tokens)

# Print to confirm the process
print(train)
print(validation)
print(test)


from tensorflow.keras.preprocessing.text import Tokenizer

# We combine the data from all splits to create the vocabulary
all_tokens = list(chain(train['tokens'], validation['tokens'], test['tokens']))

# Creating the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_tokens)

# Convert words to numerical indices
train_seq = tokenizer.texts_to_sequences(train['tokens'])
val_seq = tokenizer.texts_to_sequences(validation['tokens'])
test_seq = tokenizer.texts_to_sequences(test['tokens'])

from tensorflow.keras.preprocessing.sequence import pad_sequences

# We define the maximum length
max_length = max(max(len(seq) for seq in train_seq), max(len(seq) for seq in val_seq), max(len(seq) for seq in test_seq))

# Padding
train_padded = pad_sequences(train_seq, maxlen=max_length, padding='post')
val_padded = pad_sequences(val_seq, maxlen=max_length, padding='post')
test_padded = pad_sequences(test_seq, maxlen=max_length, padding='post')

from sklearn.preprocessing import LabelEncoder

# Encoding the tags
label_encoder = LabelEncoder()
label_encoder.fit(list(chain.from_iterable(train['ner_tags'])))

# label_encoder.classes_ contains the labels in integer format
max_label = max(label_encoder.transform(label_encoder.classes_)) + 1
print(max_label)

train_labels = [label_encoder.transform(tags) for tags in train['ner_tags']]
val_labels = [label_encoder.transform(tags) for tags in validation['ner_tags']]
test_labels = [label_encoder.transform(tags) for tags in test['ner_tags']]

# Use the max_label value for padding
train_labels_padded = pad_sequences(train_labels, maxlen=max_length, padding='post', value=max_label-1)
val_labels_padded = pad_sequences(val_labels, maxlen=max_length, padding='post', value=max_label-1)
test_labels_padded = pad_sequences(test_labels, maxlen=max_length, padding='post', value=max_label-1)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, TimeDistributed, Bidirectional

# Model parameters
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for zero padding
embed_dim = 128  # Dimension of  embedding
lstm_units = 64  # Number of units of LSTM
output_dim = len(label_encoder.classes_)  # Number of labels

# Model creation
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length, mask_zero=True),
    Bidirectional(LSTM(units=lstm_units, return_sequences=True)),
    TimeDistributed(Dense(output_dim, activation='softmax'))  # Using softmax for multiclass classification
])

# Set the loss function to ignore padding values
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'], loss_weights=[None, 1.0])

# Show model summary
model.summary()

# Model training
history = model.fit(
    train_padded,
    np.expand_dims(train_labels_padded, -1),
    validation_data=(val_padded, val_labels_padded),
    epochs=10,
    batch_size=32)


# Evaluation of the model on the test set
test_loss, test_accuracy = model.evaluate(test_padded, test_labels_padded)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')


from sklearn.metrics import classification_report
# Prediction on test set
test_predictions = model.predict(test_padded)
# Convert predictions from probability distributions to label index
test_predictions = np.argmax(test_predictions, axis=-1)

# We need to flatten the predictions and true values to suit the classification report
true_labels = test_labels_padded.flatten()
predicted_labels = test_predictions.flatten()

# Removing padding from the evaluation: This depends on your padding label, adjust 'max_label' accordingly
mask = true_labels < max_label - 1  # Assuming 'max_label - 1' is used for padding
true_labels = true_labels[mask]
predicted_labels = predicted_labels[mask]

# Generate classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print("Classification Report for full pipeline experiment:\n", report)


# Definition of preprocessing function just for tokenization

#Calling a new function that will only include tokenization
def preprocess_tokens(example):
    # Tokenization
    tokens = word_tokenize(' '.join(example['tokens']))

    example['processed_tokens'] = tokens
    return example


# Application of preprocessing functions
train = train.map(preprocess_tokens)
validation = validation.map(preprocess_tokens)
test = test.map(preprocess_tokens)

# Display the first lines from each set
print("Train set processed tokens example:", train['processed_tokens'][:3])
print("Validation set processed tokens example:", validation['processed_tokens'][:3])
print("Test set processed tokens example:", test['processed_tokens'][:3])

print(train)
print(validation)
print(test)


# Delete original 'tokens'
train = train.remove_columns('tokens')
validation = validation.remove_columns('tokens')
test = test.remove_columns('tokens')
# Print to confirm the process
print(train)
print(validation)
print(test)

from datasets import DatasetDict

# Replace 'tokens' with 'processed tokens' and rename to 'tokens'
def replace_and_rename_tokens(example):
    example['tokens'] = example.pop('processed_tokens')
    return example

# Apply the change to each dataset
train = train.map(replace_and_rename_tokens)
validation = validation.map(replace_and_rename_tokens)
test = test.map(replace_and_rename_tokens)

# Print to confirm the process
print(train)
print(validation)
print(test)


from tensorflow.keras.preprocessing.text import Tokenizer

# We combine the data from all splits to create the vocabulary
all_tokens = list(chain(train['tokens'], validation['tokens'], test['tokens']))

# Creating the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_tokens)

# Convert words to numerical indices
train_seq = tokenizer.texts_to_sequences(train['tokens'])
val_seq = tokenizer.texts_to_sequences(validation['tokens'])
test_seq = tokenizer.texts_to_sequences(test['tokens'])


from tensorflow.keras.preprocessing.sequence import pad_sequences

# We define the maximum length
max_length = max(max(len(seq) for seq in train_seq), max(len(seq) for seq in val_seq), max(len(seq) for seq in test_seq))

# Padding
train_padded = pad_sequences(train_seq, maxlen=max_length, padding='post')
val_padded = pad_sequences(val_seq, maxlen=max_length, padding='post')
test_padded = pad_sequences(test_seq, maxlen=max_length, padding='post')


from sklearn.preprocessing import LabelEncoder

# Encoding the tags
label_encoder = LabelEncoder()
label_encoder.fit(list(chain.from_iterable(train['ner_tags'])))

# label_encoder.classes_ contains the labels in integer format
max_label = max(label_encoder.transform(label_encoder.classes_)) + 1
print(max_label)

train_labels = [label_encoder.transform(tags) for tags in train['ner_tags']]
val_labels = [label_encoder.transform(tags) for tags in validation['ner_tags']]
test_labels = [label_encoder.transform(tags) for tags in test['ner_tags']]

# Use the max_label value for padding
train_labels_padded = pad_sequences(train_labels, maxlen=max_length, padding='post', value=max_label-1)
val_labels_padded = pad_sequences(val_labels, maxlen=max_length, padding='post', value=max_label-1)
test_labels_padded = pad_sequences(test_labels, maxlen=max_length, padding='post', value=max_label-1)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, TimeDistributed, Bidirectional

# Model parameters
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for zero padding
embed_dim = 128  # Dimension of  embedding
lstm_units = 64  # Number of units of LSTM
output_dim = len(label_encoder.classes_)  # Number of labels

# Model creation
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length, mask_zero=True),
    Bidirectional(LSTM(units=lstm_units, return_sequences=True)),
    TimeDistributed(Dense(output_dim, activation='softmax'))  # Using softmax for multiclass classification
])

# Set the loss function to ignore padding values
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'], loss_weights=[None, 1.0])

# Show model summary
model.summary()

# Model training
history = model.fit(
    train_padded,
    np.expand_dims(train_labels_padded, -1),
    validation_data=(val_padded, val_labels_padded),
    epochs=10,
    batch_size=32)


# Evaluation of the model on the test set
test_loss, test_accuracy = model.evaluate(test_padded, test_labels_padded)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')


# Prediction on test set
test_predictions = model.predict(test_padded)
# Convert predictions from probability distributions to label index
test_predictions = np.argmax(test_predictions, axis=-1)

# We need to flatten the predictions and true values to suit the classification report
true_labels = test_labels_padded.flatten()
predicted_labels = test_predictions.flatten()

# Removing padding from the evaluation: This depends on your padding label, adjust 'max_label' accordingly
mask = true_labels < max_label - 1  # Assuming 'max_label - 1' is used for padding
true_labels = true_labels[mask]
predicted_labels = predicted_labels[mask]

# Generate classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print("Classification Report for tokenization only experiment:\n", report)

from gensim.models import Word2Vec

# Training of Word2Vec
model_w2v = Word2Vec(sentences=train['tokens'], vector_size=100, window=5, min_count=1, workers=multiprocessing.cpu_count())

# Saving the model
model_w2v.save("word2vec.model")


from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
#Extract vectors for each word in each row of the dataset and match with the NER label
X_train = []
y_train = []

for tokens, tags in zip(train['tokens'], train['ner_tags']):
    for token, tag in zip(tokens, tags):
        if token in model_w2v.wv:  # Check if the word exists in the model dictionary
            X_train.append(model_w2v.wv[token])
            y_train.append(tag)  # Adding the corresponding NER tag

# Training of KNN
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Repeat for the test set
X_test = []
y_test = []

for tokens, tags in zip(test['tokens'], test['ner_tags']):
    for token, tag in zip(tokens, tags):
        if token in model_w2v.wv:
            X_test.append(model_w2v.wv[token])
            y_test.append(tag)

# Prediction
y_pred = knn_model.predict(X_test)

# Print accuracy and F1 score
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1}')


from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate classification report
report_knn = classification_report(y_test, y_pred, output_dict=True)

# Convert report to DataFrame
report_knn_df = pd.DataFrame(report_knn).transpose()

# Display classification report table
print(report_knn_df)

# Confusion matrix
cm_knn = confusion_matrix(y_test, y_pred)

# Plotting confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(cm_knn, annot=True, fmt='d')
plt.title('kNN Model Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.show()

# Plotting bar chart for classification metrics
report_knn_df.drop(['accuracy'], inplace=True)  # Drop the total accuracy row
report_knn_df['support'] = report_knn_df.support.apply(lambda s: int(s))  # Convert support to integer for plotting
report_knn_df.plot(kind='bar', y=['precision', 'recall', 'f1-score'], figsize=(10, 7))

plt.title('kNN Model Classification Metrics')
plt.ylabel('Score')
plt.xlabel('Classes')
plt.xticks(rotation=45)
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()


import gensim.downloader as api

# We load the pretrained GloVe model
glove_model = api.load("glove-wiki-gigaword-100")  # It uses 100 dimensional vectors

# Create embedding matrix as previously described
embedding_dim = 100  # Dimension according to GloVe model used above
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    try:
        embedding_vector = glove_model[word]  # Search the vector for the word
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    except KeyError:
        continue  # Word not found in GloVe model

# We integrate the embedding matrix into the model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(units=64, return_sequences=True)),
    TimeDistributed(Dense(output_dim, activation='softmax'))
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Training the model
history = model.fit(
    train_padded,
    np.expand_dims(train_labels_padded, -1),  # The labels on the proper form
    validation_data=(val_padded, np.expand_dims(val_labels_padded, -1)),
    epochs=10,
    batch_size=32
)

# Illustrating the evolution of training
plt.figure(figsize=(12, 8))
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Evolution of training')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(12, 8))
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Evolution of Loss during Education')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluation of the model on the test set
test_loss, test_accuracy = model.evaluate(test_padded, np.expand_dims(test_labels_padded, -1))
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')

# Prediction on the test set and illustration of examples
predictions = model.predict(test_padded)
predicted_labels = np.argmax(predictions, axis=-1)

# Random samples to demonstrate predictions
indices = np.random.choice(range(len(test_labels)), 5, replace=False)
for idx in indices:
    print("Real labels:", label_encoder.inverse_transform(test_labels[idx]))
    print("Predicted labels:", label_encoder.inverse_transform(predicted_labels[idx]))
    print("\n")


import tensorflow as tf
from tensorflow.keras.metrics import Accuracy, Precision, Recall

# Convert logits to class predictions
predicted_labels = tf.argmax(predictions, axis=-1)

# Prepare a metric object for accuracy and calculate
accuracy_metric = Accuracy()
accuracy_metric.update_state(test_labels_padded, predicted_labels)
test_accuracy = accuracy_metric.result().numpy()

# For F1-score, we need both precision and recall
precision_metric = Precision()
recall_metric = Recall()
precision_metric.update_state(test_labels_padded, predicted_labels)
recall_metric.update_state(test_labels_padded, predicted_labels)
precision = precision_metric.result().numpy()
recall = recall_metric.result().numpy()

# Calculate F1 score
if (precision + recall) != 0:
    test_f1_score = 2 * (precision * recall) / (precision + recall)
else:
    test_f1_score = 0

# Print results
print(f'Test Accuracy: {test_accuracy}')
print(f'Test Precision: {precision}')
print(f'Test Recall: {recall}')
print(f'Test F1 Score: {test_f1_score}')


import gensim.downloader as api

# We load the pretrained GloVe model
glove_model = api.load("glove-wiki-gigaword-100")  # It uses 100 dimensional vectors

# Create embedding matrix as previously described
embedding_dim = 100  # Dimension according to GloVe model used above
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    try:
        embedding_vector = glove_model[word]  # Search the vector for the word
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    except KeyError:
        continue  # Word not found in GloVe model

# We integrate the embedding matrix into the model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(units=64, return_sequences=True)),
    TimeDistributed(Dense(output_dim, activation='softmax'))
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

import gensim.downloader as api
import numpy as np
from sklearn.neighbors import KNeighborsClassifier

# Loading the GloVe model
glove_model = api.load("glove-wiki-gigaword-100")

# Creation of training data
X_train = []
y_train = []

for tokens, tags in zip(train['tokens'], train['ner_tags']):
    for token, tag in zip(tokens, tags):
        token = token.lower()
        if token in glove_model:
            X_train.append(glove_model[token])
            y_train.append(tag)

# Training of KNN
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Data preparation and evaluation
X_test = []
y_test = []

for tokens, tags in zip(test['tokens'], test['ner_tags']):
    for token, tag in zip(tokens, tags):
        token = token.lower()
        if token in glove_model:
            X_test.append(glove_model[token])
            y_test.append(tag)

# Prediction
y_pred = knn_model.predict(X_test)

# Print accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1}')

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate classification report
report_knn = classification_report(y_test, y_pred, output_dict=True)

# Convert report to DataFrame
report_knn_df = pd.DataFrame(report_knn).transpose()

# Display classification report table
print(report_knn_df)

# Confusion matrix
cm_knn = confusion_matrix(y_test, y_pred)

# Plotting confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(cm_knn, annot=True, fmt='d')
plt.title('kNN Model Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.show()

# Plotting bar chart for classification metrics
report_knn_df.drop(['accuracy'], inplace=True)  # Drop the total accuracy row
report_knn_df['support'] = report_knn_df.support.apply(lambda s: int(s))  # Convert support to integer for plotting
report_knn_df.plot(kind='bar', y=['precision', 'recall', 'f1-score'], figsize=(10, 7))

plt.title('kNN Model Classification Metrics')
plt.ylabel('Score')
plt.xlabel('Classes')
plt.xticks(rotation=45)
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()


# Training of KNN
knn_model = KNeighborsClassifier(n_neighbors=100)
knn_model.fit(X_train, y_train)

# Data preparation and evaluation
X_test = []
y_test = []

for tokens, tags in zip(test['tokens'], test['ner_tags']):
    for token, tag in zip(tokens, tags):
        token = token.lower()
        if token in glove_model:
            X_test.append(glove_model[token])
            y_test.append(tag)

# Prediction
y_pred = knn_model.predict(X_test)

# Print accuracy and f1 score
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1}')


import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Prepare your test data outside the loop to optimize performance
X_test = []
y_test = []
for tokens, tags in zip(test['tokens'], test['ner_tags']):
    for token, tag in zip(tokens, tags):
        token = token.lower()
        if token in glove_model:
            X_test.append(glove_model[token])
            y_test.append(tag)

# Store accuracies for different values of n_neighbors
accuracies = []
neighbor_range = range(1, 31)  # n_neighbors from 1 to 30

for n_neighbors in neighbor_range:
    # Training of KNN
    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn_model.fit(X_train, y_train)

    # Prediction
    y_pred = knn_model.predict(X_test)

    # Calculate and print accuracy and f1 score
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f'Accuracy for {n_neighbors} neighbors: {accuracy:.2%}')
    f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1}')
# Plotting the accuracies
plt.figure(figsize=(10, 6))
plt.plot(neighbor_range, accuracies, marker='o', linestyle='-', color='b')
plt.title('KNN Accuracy vs. Number of Neighbors')
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.grid(True)
plt.xticks(neighbor_range)  # Ensure all neighbor values are marked
plt.show()





